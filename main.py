"""
ULTIMATE ROMAJI ENGINE (v33.0-ENHANCED)
"The Ultra Anti-Hallucination Edition"

NEW IN v33:
- ENHANCED VALIDATION: Post-AI checks to catch hallucinations
- COMPOUND PRIORITY: Special handling for tricky compounds
- DETAILED LOGGING: Track all conversions and changes
- TEST SUITE: Built-in quality assurance
- STRICTER AI PROMPTS: Better instructions with examples
- CONFIDENCE SCORING: Know when to trust the output
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from openai import AsyncOpenAI
import os
import re
import hashlib
import json
import redis.asyncio as redis
import asyncio
import time
from typing import List, Optional, Dict, Any, Tuple
import logging
import urllib.parse
import aiohttp
import requests
import gzip
import sqlite3
import gc
from bs4 import BeautifulSoup
from difflib import SequenceMatcher
from contextlib import asynccontextmanager

# ===== LOGGING =====
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger("RomajiGuardian")

# ===== LIFECYCLE =====
@asynccontextmanager
async def lifespan(app: FastAPI):
    asyncio.create_task(background_builder())
    yield

app = FastAPI(title="Romaji Guardian Enhanced", version="33.0.0", lifespan=lifespan)
app.add_middleware(
    CORSMiddleware, 
    allow_origins=["*"], 
    allow_credentials=True, 
    allow_methods=["*"], 
    allow_headers=["*"]
)

# ===== CONFIG =====
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
REDIS_URL = os.environ.get("REDIS_URL")
ADMIN_SECRET = "admin123"
DB_FILE = "titan_universe.db"
DB_VERSION_MARKER = "v33_enhanced"

MODELS = {
    "deepseek": {
        "id": "deepseek-chat",
        "client": None,
        "base": "https://api.deepseek.com",
        "key": DEEPSEEK_API_KEY
    },
    "groq": {
        "id": "llama-3.3-70b-versatile",
        "client": None,
        "base": "https://api.groq.com/openai/v1",
        "key": GROQ_API_KEY
    }
}

# ===== COMPOUND PRIORITY =====
# These MUST be kept together as single units
COMPOUND_PRIORITY = {
    "Â∞èÈÉ®Â±ã": "kobeya",      # NOT "ko heya"
    "ÁÖôËçâ": "tabako",        # NOT "kemuri kusa"
    "„Åü„Å∞„Åì": "tabako",
    "„Çø„Éê„Ç≥": "tabako",
    "‰ªäÊó•": "ky≈ç",           # NOT "konnichi"
    "ÊòéÊó•": "ashita",        # NOT "my≈çnichi"  
    "Êò®Êó•": "kin≈ç",          # NOT "sakujitsu"
    "‰ªä": "ima",             # NOT "genzai" or "kon"
    "‰ªäÂ§ú": "kon'ya",
    "‰ªäÊúù": "kesa",
    "‰ªäÂπ¥": "kotoshi",
    "ÂéªÂπ¥": "kyonen",
    "Êù•Âπ¥": "rainen",
    "ÂÖàÊúà": "sengetsu",
    "‰ªäÊúà": "kongetsu",
    "Êù•Êúà": "raigetsu",
}

# ===== LYRIC PACK (LOCKED WORDS) =====
LYRIC_PACK = {
    "ÈÅãÂëΩ": "unmei", "Â•áË∑°": "kiseki", "Á¥ÑÊùü": "yakusoku", "Ë®òÊÜ∂": "kioku",
    "Áâ©Ë™û": "monogatari", "‰ºùË™¨": "densetsu", "Ê∞∏ÈÅ†": "eien", "Áû¨Èñì": "shunkan",
    "Ë°ùÂãï": "sh≈çd≈ç", "ÊÆãÈÖ∑": "zankoku", "Â§©‰Ωø": "tenshi", "Á•ûË©±": "shinwa",
    "ÈºìÂãï": "kod≈ç", "ÊóãÂæã": "senritsu", "ÂÖ±È≥¥": "ky≈çmei", "ÂπªÊÉ≥": "gens≈ç",
    "Ê•ΩÂúí": "rakuen", "ÊñπËàü": "hakobune", "ÈªÑÊòè": "tasogare", "ÈªéÊòé": "reimei",
    "ÂàπÈÇ£": "setsuna", "ÊÇ†‰πÖ": "y≈´ky≈´", "ÂΩºÊñπ": "kanata", "Ê≥°Ê≤´": "utakata",
    "Ëû∫Êóã": "rasen", "Âõ†Êûú": "inga", "Ëº™Âªª": "rinne", "Ë¶öÈÜí": "kakusei",
    "ÂíÜÂìÆ": "h≈çk≈ç", "ÊÆãÈüø": "zanky≈ç", "Áµ∂Êúõ": "zetsub≈ç", "Â∏åÊúõ": "kib≈ç",
    "ÁµÜ": "kizuna", "Ë®º": "akashi", "Áøº": "tsubasa", "Êââ": "tobira",
    "Èçµ": "kagi", "Èéñ": "kusari", "ÁÇé": "hon≈ç", "Ê∞∑": "k≈çri",
    "ÂÖâ": "hikari", "Èóá": "yami", "ÂΩ±": "kage", "Á©∫": "sora",
    "Êµ∑": "umi", "Êòü": "hoshi", "Êúà": "tsuki", "Â§¢": "yume",
    "ÁßÅ": "watashi", "ÂÉï": "boku", "‰ø∫": "ore", "Âêõ": "kimi", "Ë≤¥Êñπ": "anata",
    "‰∏ñÁïå": "sekai", "Ë®ÄËëâ": "kotoba", "ÂøÉ": "kokoro", "ÊÑõ": "ai", 
    "Ê∂ô": "namida", "Á¨ëÈ°î": "egao", "Áû≥": "hitomi", 
    "Ê≠å": "uta", "Á¢∫‰ø°": "kakushin", "Èáç„Å≠„Å¶": "kasanete",
    **COMPOUND_PRIORITY  # Merge compound priority into lyric pack
}

# ===== HALLUCINATION PATTERNS =====
# Common wrong conversions the AI might make
HALLUCINATION_PAIRS = [
    ("ima", "genzai"),       # ‰ªä should be 'ima' in conversation
    ("ky≈ç", "honjitsu"),     # ‰ªäÊó• conversational vs formal
    ("ashita", "my≈çnichi"),  # ÊòéÊó• conversational vs formal
    ("kin≈ç", "sakujitsu"),   # Êò®Êó• conversational vs formal
    ("tabako", "kemuri"),    # Don't split ÁÖôËçâ
    ("kobeya", "ko heya"),   # Don't split Â∞èÈÉ®Â±ã
]

# ===== BUILDER =====
EDICT_URL = "http://ftp.edrdg.org/pub/Nihongo/edict.gz"
ENAMDICT_URL = "http://ftp.edrdg.org/pub/Nihongo/enamdict.gz"

def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('CREATE TABLE IF NOT EXISTS dictionary (word TEXT PRIMARY KEY, romaji TEXT)')
    c.execute('CREATE TABLE IF NOT EXISTS meta (key TEXT PRIMARY KEY, value TEXT)')
    conn.commit()
    return conn

async def download_and_parse_stream(url, label, converter, conn):
    try:
        temp_file = f"temp_{label}.gz"
        logger.info(f"‚¨áÔ∏è {label}: Downloading...")
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                if resp.status == 200:
                    with open(temp_file, 'wb') as f:
                        while True:
                            chunk = await resp.content.read(1024*1024)
                            if not chunk: break
                            f.write(chunk)
                else: return

        logger.info(f"üì¶ {label}: Parsing...")
        c = conn.cursor()
        batch = []
        
        with gzip.open(temp_file, 'rt', encoding='euc-jp', errors='ignore') as f:
            for line in f:
                try:
                    if " [" in line:
                        parts = line.split(" [", 1)
                        word, rest = parts[0], parts[1]
                        reading = rest.split("]", 1)[0] if "]" in rest else word
                    else:
                        word = line.split(" /", 1)[0]
                        reading = word

                    if word and reading:
                        if len(word) == 1 and ('\u3040' <= word <= '\u309f'): continue 
                        if any(x in word for x in "‚ñΩ‚ñº().,"): continue
                        
                        romaji = converter.do(reading).strip()
                        if ";" in word:
                            for w in word.split(";"):
                                clean = w.split("(")[0].strip()
                                if clean: batch.append((clean, romaji))
                        else:
                            clean = word.split("(")[0].strip()
                            batch.append((clean, romaji))
                        
                        if len(batch) >= 20000:
                            c.executemany('INSERT OR IGNORE INTO dictionary VALUES (?,?)', batch)
                            conn.commit()
                            batch = []
                            await asyncio.sleep(0.01)
                except: continue
        
        if batch:
            c.executemany('INSERT OR IGNORE INTO dictionary VALUES (?,?)', batch)
            conn.commit()
        
        if os.path.exists(temp_file): os.remove(temp_file)
        logger.info(f"‚úÖ {label}: Done.")
    except Exception as e: 
        logger.error(f"‚ö†Ô∏è {label} Error: {e}")

async def background_builder():
    if check_db_status() > 50000: return
    logger.info("üöÄ BUILDER STARTED")
    import pykakasi
    k = pykakasi.kakasi()
    k.setMode("H", "a")
    k.setMode("K", "a")
    k.setMode("J", "a")
    k.setMode("r", "Hepburn")
    conv = k.getConverter()
    
    if os.path.exists(DB_FILE): os.remove(DB_FILE)
    conn = init_db()
    
    await download_and_parse_stream(EDICT_URL, "EDICT", conv, conn)
    await download_and_parse_stream(ENAMDICT_URL, "ENAMDICT", conv, conn)
    
    c = conn.cursor()
    c.execute("INSERT OR REPLACE INTO meta VALUES ('version', ?)", (DB_VERSION_MARKER,))
    conn.commit()
    conn.close()
    logger.info("üéâ BUILD COMPLETE")

def check_db_status():
    if not os.path.exists(DB_FILE): return 0
    try:
        with sqlite3.connect(DB_FILE) as conn:
            c = conn.cursor()
            c.execute("SELECT value FROM meta WHERE key='version'")
            ver = c.fetchone()
            if not ver or ver[0] != DB_VERSION_MARKER: return -1
            c.execute('SELECT count(*) FROM dictionary')
            return c.fetchone()[0]
    except: return 0

def get_static_romaji(word):
    if len(word) < 2: return None 
    try:
        with sqlite3.connect(DB_FILE) as conn:
            c = conn.cursor()
            c.execute('SELECT romaji FROM dictionary WHERE word=?', (word,))
            res = c.fetchone()
            if res: return res[0]
    except: return None

# ===== INIT =====
redis_client = None
tagger = None
kakasi_conv = None
l1_cache = {}

def init_globals():
    global tagger, kakasi_conv, redis_client, MODELS
    try:
        import fugashi
        import unidic_lite
        tagger = fugashi.Tagger(f'-d {unidic_lite.DICDIR}')
    except: pass
    try:
        import pykakasi
        k = pykakasi.kakasi()
        k.setMode("H", "a")
        k.setMode("K", "a")
        k.setMode("J", "a")
        k.setMode("r", "Hepburn")
        kakasi_conv = k.getConverter()
    except: pass
    for name, conf in MODELS.items():
        if conf["key"]:
            conf["client"] = AsyncOpenAI(api_key=conf["key"], base_url=conf["base"])
    if REDIS_URL:
        try:
            redis_client = redis.from_url(REDIS_URL, decode_responses=True)
        except: pass

init_globals()

# ===== SEARCH =====
class LyricSearchEngine:
    BASE_URL = "http://search.j-lyric.net/index.php"
    
    @staticmethod
    async def find_official_line(session, user_text):
        clean_q = re.sub(r"[!?.„ÄÅ„ÄÇ]", " ", user_text).strip()
        if len(clean_q) < 4: return None
        params = {"kt": clean_q, "ct": 2, "ka": "", "ca": 2, "kl": "", "cl": 2}
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        try:
            async with session.get(
                LyricSearchEngine.BASE_URL, 
                params=params, 
                headers=headers, 
                timeout=4.0
            ) as resp:
                if resp.status != 200: return None
                soup = BeautifulSoup(await resp.text(), 'html.parser')
                body = soup.find('div', id='mnb')
                if not body: return None
                link = body.find('p', class_='mid').find('a')
                if not link: return None
                
                async with session.get(link['href'], headers=headers, timeout=4.0) as song_resp:
                    song_soup = BeautifulSoup(await song_resp.text(), 'html.parser')
                    lyric_div = song_soup.find('p', id='Lyric')
                    if lyric_div:
                        for line in lyric_div.get_text(separator="\n").split("\n"):
                            if SequenceMatcher(None, user_text, line).ratio() > 0.6:
                                return line
        except: 
            return None
        return None

# ===== GLUE & LOCKING =====

def local_convert(text: str) -> Tuple[str, List[str], Dict[str, str]]:
    """
    Convert Japanese to Romaji with enhanced glue logic and locking.
    Returns: (romaji_draft, research_targets, locked_words)
    """
    if not tagger or not kakasi_conv: 
        return text, [], {}
    
    romaji_parts = []
    research_targets = []
    locked_words = {}  # Words that MUST NOT be changed by AI
    
    text = text.replace("„ÄÄ", " ")
    nodes = list(tagger(text))
    i = 0
    
    while i < len(nodes):
        node = nodes[i]
        word = node.surface
        
        # PRIORITY: Check COMPOUND_PRIORITY first (before any other glue)
        # This ensures compounds like Â∞èÈÉ®Â±ã stay together
        for length in [3, 2]:  # Check 3-char then 2-char compounds
            if i + length - 1 < len(nodes):
                combo = "".join([nodes[i+j].surface for j in range(length)])
                if combo in COMPOUND_PRIORITY:
                    r = COMPOUND_PRIORITY[combo]
                    romaji_parts.append(r)
                    locked_words[combo] = r
                    logger.debug(f"üîí Compound Priority: {combo} -> {r}")
                    i += length
                    break
            if length == 2:  # No compound found, continue to normal processing
                break
        else:
            i += 1
            continue

        # 3-LEVEL GLUE (for non-priority compounds)
        if i + 2 < len(nodes):
            tri_combo = word + nodes[i+1].surface + nodes[i+2].surface
            if tri_combo in LYRIC_PACK:
                r = LYRIC_PACK[tri_combo]
                romaji_parts.append(r)
                locked_words[tri_combo] = r
                i += 3
                continue
            db_hit = get_static_romaji(tri_combo)
            if db_hit:
                romaji_parts.append(db_hit)
                locked_words[tri_combo] = db_hit
                i += 3
                continue

        # 2-LEVEL GLUE
        if i + 1 < len(nodes):
            duo_combo = word + nodes[i+1].surface
            if duo_combo in LYRIC_PACK:
                r = LYRIC_PACK[duo_combo]
                romaji_parts.append(r)
                locked_words[duo_combo] = r
                i += 2
                continue
            db_hit = get_static_romaji(duo_combo)
            if db_hit:
                romaji_parts.append(db_hit)
                locked_words[duo_combo] = db_hit
                i += 2
                continue

        # PARTICLES
        if node.feature[0] == 'Âä©Ë©û':
            if word == '„ÅØ': 
                romaji_parts.append('wa')
            elif word == '„Å∏': 
                romaji_parts.append('e')
            elif word == '„Çí': 
                romaji_parts.append('wo')
            else: 
                romaji_parts.append(kakasi_conv.do(word))
            i += 1
            continue
            
        # LYRIC PACK (individual words)
        if word in LYRIC_PACK:
            r = LYRIC_PACK[word]
            romaji_parts.append(r)
            locked_words[word] = r
            logger.debug(f"üîí Lyric Pack: {word} -> {r}")
            i += 1
            continue

        # DATABASE + MECAB
        db_romaji = get_static_romaji(word)
        mecab_raw = node.feature[7] if len(node.feature) > 7 and node.feature[7] != '*' else word
        mecab_romaji = kakasi_conv.do(mecab_raw).strip()
        
        if db_romaji:
            # Phonetic Guard: For single chars, check if DB is wildly different
            if len(word) == 1:
                similarity = SequenceMatcher(None, db_romaji, mecab_romaji).ratio()
                if similarity < 0.3:
                    # DB is too different, trust MeCab but lock it
                    romaji_parts.append(mecab_romaji)
                    locked_words[word] = mecab_romaji
                    logger.debug(f"üîí MeCab Override: {word} -> {mecab_romaji} (DB: {db_romaji})")
                else:
                    romaji_parts.append(db_romaji)
                    locked_words[word] = db_romaji
            else:
                romaji_parts.append(db_romaji)
                locked_words[word] = db_romaji
        else:
            romaji_parts.append(mecab_romaji)
            # Lock single kanji even from MeCab to prevent AI hallucination
            if len(word) == 1 and any('\u4e00' <= c <= '\u9fff' for c in word):
                locked_words[word] = mecab_romaji

        # Track words that need research
        if any('\u4e00' <= c <= '\u9fff' for c in word):
            research_targets.append(word)
        
        i += 1

    draft = re.sub(r'\s+', ' ', " ".join(romaji_parts)).strip()
    return draft, list(set(research_targets)), locked_words

# ===== VALIDATION =====

def validate_romaji(original_jp: str, draft: str, final: str, locked: Dict) -> Tuple[str, List[str]]:
    """
    Validate AI output against locked words and hallucination patterns.
    Returns: (validated_romaji, list_of_warnings)
    """
    warnings = []
    validated = final
    
    # Check 1: Ensure locked words are still present
    for jp_word, expected_rom in locked.items():
        if jp_word in original_jp:
            # Normalize for comparison
            expected_lower = expected_rom.lower().replace("'", "").replace("-", "")
            draft_lower = draft.lower().replace("'", "").replace("-", "")
            final_lower = final.lower().replace("'", "").replace("-", "")
            
            if expected_lower in draft_lower and expected_lower not in final_lower:
                warnings.append(f"Locked word missing: '{jp_word}' -> '{expected_rom}'")
                validated = draft
                logger.warning(f"‚ö†Ô∏è Validation: Locked word '{expected_rom}' for '{jp_word}' missing in AI output. Reverting to draft.")
                return validated, warnings
    
    # Check 2: Detect common hallucinations
    for correct, wrong in HALLUCINATION_PAIRS:
        draft_has_correct = correct in draft.lower()
        final_has_wrong = wrong in final.lower()
        final_missing_correct = correct not in final.lower()
        
        if draft_has_correct and final_has_wrong and final_missing_correct:
            warnings.append(f"Hallucination: {correct} -> {wrong}")
            validated = draft
            logger.warning(f"‚ö†Ô∏è Hallucination detected: {correct} became {wrong}. Reverting to draft.")
            return validated, warnings
    
    # Check 3: Ensure no major length change (AI didn't go crazy)
    draft_words = len(draft.split())
    final_words = len(final.split())
    if abs(draft_words - final_words) > max(3, draft_words * 0.3):
        warnings.append(f"Length mismatch: {draft_words} words -> {final_words} words")
        validated = draft
        logger.warning(f"‚ö†Ô∏è Major length change detected. Reverting to draft.")
        return validated, warnings
    
    return validated, warnings

def log_conversion_details(text: str, draft: str, final: str, locked: Dict, method: str, warnings: List[str]):
    """Log detailed conversion information for debugging"""
    if draft != final or warnings:
        logger.info(f"üìù CONVERSION for '{text[:50]}...'")
        logger.info(f"   Draft:  {draft}")
        logger.info(f"   Final:  {final}")
        logger.info(f"   Method: {method}")
        logger.info(f"   Locked: {len(locked)} words - {list(locked.keys())[:5]}")
        if warnings:
            logger.info(f"   ‚ö†Ô∏è Warnings: {warnings}")

# ===== AI PROCESSING =====

async def call_ai(client, model_id, prompt):
    """Call AI with enhanced parameters for consistency"""
    try:
        resp = await client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model=model_id, 
            temperature=0.0,
            top_p=0.1,  # More deterministic
            response_format={"type": "json_object"}, 
            timeout=8.0
        )
        return json.loads(resp.choices[0].message.content)
    except Exception as e:
        logger.error(f"AI call failed: {e}")
        return None

async def process_text_guardian(text: str) -> Dict:
    """Main processing pipeline with enhanced validation"""
    start = time.perf_counter()
    cache_key = f"guardian_v33:{hashlib.md5(text.encode()).hexdigest()}"
    
    # Check L1 cache
    if cache_key in l1_cache: 
        return l1_cache[cache_key]
    
    # Check Redis cache
    if redis_client:
        try:
            cached = await redis_client.get(cache_key)
            if cached: 
                return json.loads(cached)
        except: pass

    # Step 1: Local conversion with locking
    draft, research_needs, locked = local_convert(text)
    final_romaji = draft
    method = "local_db"
    official_match = False
    warnings = []
    confidence = "high"
    
    # Step 2: Try to find official lyric match
    async with aiohttp.ClientSession() as session:
        official = await LyricSearchEngine.find_official_line(session, text)
        if official:
            draft_off, _, _ = local_convert(official)
            final_romaji = draft_off
            method = "official_match"
            official_match = True
            logger.info(f"‚úÖ Found official match for: {text[:30]}...")
    
    # Step 3: AI refinement (only if needed and no official match)
    if research_needs and not official_match and MODELS["deepseek"]["client"]:
        # Build comprehensive lock list
        lock_examples = []
        for jp, rom in list(locked.items())[:8]:  # Show 8 examples
            lock_examples.append(f"  ‚Ä¢ '{jp}' = '{rom}'")
        
        # Create detailed prompt with explicit examples
        prompt = f"""You are an expert Romaji correction system. Your ONLY job is to fix obvious errors.

ORIGINAL JAPANESE: {text}
INITIAL ROMAJI: {draft}

üîí CRITICAL - THESE WORDS ARE LOCKED (NEVER CHANGE):
{chr(10).join(lock_examples)}

Full locked map: {json.dumps(locked, ensure_ascii=False)}

üìã RULES (STRICT):
1. Fix ONLY particles: „ÅØ‚Üíwa, „Å∏‚Üíe, „Çí‚Üíwo
2. Fix ONLY spacing issues
3. NEVER change any locked word
4. Prefer conversational readings:
   - ‰ªä = "ima" NOT "genzai"
   - ‰ªäÊó• = "ky≈ç" NOT "honjitsu" 
   - ÊòéÊó• = "ashita" NOT "my≈çnichi"
   - Â∞èÈÉ®Â±ã = "kobeya" NOT "ko heya"
5. If unsure, DON'T change it

‚úÖ EXAMPLES:
Before: "watashiha ima ikimashou"
After: "watashi wa ima ikimashou" (only fixed particle „ÅØ‚Üíwa)

Before: "ky≈ç tabako wo suu"
After: "ky≈ç tabako wo suu" (no changes needed)

Return JSON: {{"corrected": "final romaji", "changes": ["list what you changed"], "confidence": "high/medium/low"}}"""
        
        ai_data = await call_ai(MODELS["deepseek"]["client"], MODELS["deepseek"]["id"], prompt)
        
        if ai_data:
            ai_output = ai_data.get("corrected", draft)
            ai_changes = ai_data.get("changes", [])
            confidence = ai_data.get("confidence", "medium")
            
            # CRITICAL: Validate AI output
            validated, val_warnings = validate_romaji(text, draft, ai_output, locked)
            warnings.extend(val_warnings)
            
            if not val_warnings:
                # AI output passed validation
                final_romaji = validated
                method = "deepseek_validated"
                logger.info(f"‚úÖ AI refinement accepted: {ai_changes}")
            else:
                # AI output failed validation, using draft
                final_romaji = draft
                method = "local_db_validated"
                confidence = "low"
                logger.warning(f"‚ùå AI refinement rejected: {val_warnings}")
    
    # Final cleanup
    final_romaji = re.sub(r'\s+', ' ', final_romaji).strip()
    
    # Log details
    log_conversion_details(text, draft, final_romaji, locked, method, warnings)
    
    # Build result
    result = {
        "original": text,
        "romaji": final_romaji,
        "method": method,
        "confidence": confidence,
        "warnings": warnings,
        "locked_words": len(locked),
        "time": round(time.perf_counter() - start, 4)
    }
    
    # Cache result
    l1_cache[cache_key] = result
    if redis_client:
        try:
            await redis_client.setex(cache_key, 604800, json.dumps(result))
        except: pass
    
    return result

# ===== TEST SUITE =====

TEST_CASES = [
    ("‰ªä„ÅØÁÑ°ÁêÜ", "ima wa muri"),
    ("Â∞èÈÉ®Â±ã„Å´ÂÖ•„Çã", "kobeya ni hairu"),
    ("‰ªäÊó•„ÅØÊô¥„Çå", "ky≈ç wa hare"),
    ("ÁÖôËçâ„ÇíÂê∏„ÅÜ", "tabako wo suu"),
    ("ÊòéÊó•‰ºö„ÅÑ„Åæ„Åó„Çá„ÅÜ", "ashita aimash≈ç"),
    ("ÈÅãÂëΩ„ÅÆ‰∫∫", "unmei no hito"),
    ("Á¥ÑÊùü„ÇíÂÆà„Çã", "yakusoku wo mamoru"),
    ("ÁßÅ„ÅØÂ≠¶Áîü„Åß„Åô", "watashi wa gakusei desu"),
    ("Âêõ„ÅÆÂêçÂâç„ÅØ", "kimi no namae wa"),
    ("ÂøÉ„Åã„ÇâÊÑüË¨ù", "kokoro kara kansha"),
]

@app.get("/test")
async def run_tests():
    """Run test suite to measure quality"""
    results = []
    passed_count = 0
    
    for jp, expected in TEST_CASES:
        result = await process_text_guardian(jp)
        actual = result["romaji"].lower().replace("'", "").replace("-", "")
        expected_norm = expected.lower().replace("'", "").replace("-", "")
        
        passed = actual == expected_norm
        if passed:
            passed_count += 1
        
        results.append({
            "input": jp,
            "expected": expected,
            "got": result["romaji"],
            "passed": passed,
            "method": result["method"],
            "confidence": result.get("confidence", "unknown"),
            "warnings": result.get("warnings", [])
        })
    
    return {
        "total": len(TEST_CASES),
        "passed": passed_count,
        "failed": len(TEST_CASES) - passed_count,
        "pass_rate": f"{(passed_count/len(TEST_CASES)*100):.1f}%",
        "results": results
    }

# ===== API ENDPOINTS =====

@app.get("/convert")
async def convert(text: str):
    """Convert single Japanese text to Romaji"""
    if not text: 
        raise HTTPException(400, "Text parameter required")
    return await process_text_guardian(text)

@app.post("/convert-batch")
async def convert_batch(lines: List[str]):
    """Convert multiple lines in parallel"""
    return await asyncio.gather(*[process_text_guardian(l) for l in lines])

@app.get("/lookup")
def lookup(word: str):
    """Debug: Look up a word in database and manual dict"""
    return {
        "word": word,
        "db_romaji": get_static_romaji(word),
        "manual_romaji": LYRIC_PACK.get(word),
        "compound_priority": COMPOUND_PRIORITY.get(word),
        "in_lyric_pack": word in LYRIC_PACK
    }

@app.post("/force-rebuild")
async def force_rebuild(secret: str):
    """Admin: Force database rebuild"""
    if secret != ADMIN_SECRET: 
        raise HTTPException(403, "Unauthorized")
    if os.path.exists(DB_FILE): 
        os.remove(DB_FILE)
    asyncio.create_task(background_builder())
    return {"status": "Background Rebuild Started"}

@app.post("/clear-cache")
async def clear_cache(secret: str):
    """Admin: Clear all caches"""
    if secret != ADMIN_SECRET: 
        raise HTTPException(403, "Unauthorized")
    global l1_cache
    l1_cache = {}
    if redis_client: 
        try:
            await redis_client.flushdb()
        except: pass
    return {"status": "Cache Cleared"}

@app.get("/stats")
def stats():
    """Get system statistics"""
    return {
        "version": "33.0.0",
        "db_size": check_db_status(),
        "cache_size": len(l1_cache),
        "lyric_pack_size": len(LYRIC_PACK),
        "compound_priority_size": len(COMPOUND_PRIORITY),
        "models_available": [k for k, v in MODELS.items() if v["client"]],
        "redis_connected": redis_client is not None
    }

@app.get("/")
def root():
    """Health check"""
    return {
        "status": "GUARDIAN_ENHANCED_ONLINE",
        "version": "33.0.0", 
        "db_size": check_db_status()
    }

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 10000))
    uvicorn.run(app, host="0.0.0.0", port=port)
